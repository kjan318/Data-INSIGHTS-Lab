{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1sE7upcXoddMpF5IsPC4kFzaePvu0nWKU",
      "authorship_tag": "ABX9TyN7DqOhVbWEbBdwiJRQ11Yp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kjan318/Data-INSIGHTS-Lab/blob/main/People_Analytics_dashboard_Adv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "streamlit\n",
        "pandas\n",
        "plotly\n",
        "Faker\n",
        "scikit-learn\n",
        "gspread\n",
        "oauth2client\n",
        "pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0Ql89G9JM9k",
        "outputId": "66d4f1ea-6fa6-442a-915a-b80d0f5549ab"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile google_sheets_handler.py\n",
        "import gspread\n",
        "import pandas as pd\n",
        "from google.colab import userdata\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "\n",
        "def get_gspread_client():\n",
        "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/spreadsheets\", \"https://www.googleapis.com/auth/drive.file\", \"https://www.googleapis.com/auth/drive\"]\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
        "    client = gspread.authorize(creds)\n",
        "    return client\n",
        "\n",
        "def get_spreadsheet():\n",
        "    client = get_gspread_client()\n",
        "    try:\n",
        "        sheet_key = userdata.get('GGL_SHEET_KEY')\n",
        "    except:\n",
        "        sheet_key = os.environ.get('GGL_SHEET_KEY')\n",
        "    return client.open_by_key(sheet_key)\n",
        "\n",
        "def write_df_to_sheet_tab(spreadsheet, tab_name, df):\n",
        "    \"\"\"\n",
        "    Writes a pandas DataFrame to a specific tab in a Google Sheet.\n",
        "    It will create the tab if it doesn't exist, or clear it if it does.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        worksheet = spreadsheet.worksheet(tab_name)\n",
        "        # --- KEY STEP: This line deletes all existing data in the tab ---\n",
        "        print(f\"Found existing tab '{tab_name}'. Clearing all data before writing...\")\n",
        "        worksheet.clear()\n",
        "    except gspread.WorksheetNotFound:\n",
        "        print(f\"Creating new tab: '{tab_name}'...\")\n",
        "        worksheet = spreadsheet.add_worksheet(title=tab_name, rows=\"1\", cols=\"1\")\n",
        "\n",
        "    # Write the new data to the now-empty sheet\n",
        "    worksheet.update([df.columns.values.tolist()] + df.astype(str).values.tolist())\n",
        "    print(f\"Successfully wrote new data to tab: '{tab_name}'\")\n",
        "\n",
        "def read_sheet_tab_to_df(spreadsheet, tab_name):\n",
        "    worksheet = spreadsheet.worksheet(tab_name)\n",
        "    data = worksheet.get_all_records()\n",
        "    return pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuaUixZuwFoU",
        "outputId": "7bd44aec-6f88-49a1-c6d6-9f2a9c253377"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting google_sheets_handler.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile initial_data_generator.py\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "import random\n",
        "from datetime import date, timedelta\n",
        "import google_sheets_handler as gsh\n",
        "\n",
        "def generate_raw_data(num_employees=250):\n",
        "    print(\"Starting raw data generation with complete schema...\")\n",
        "    fake = Faker()\n",
        "\n",
        "    employees_data = []\n",
        "    for i in range(num_employees):\n",
        "        emp_id = 1000 + i\n",
        "        hire_date = fake.date_between(start_date='-4y', end_date='today')\n",
        "        is_terminated = random.random() < 0.25 # 25% of employees have a termination date\n",
        "        term_date = fake.date_between(start_date=hire_date, end_date='today') if is_terminated else None\n",
        "\n",
        "        employees_data.append({\n",
        "            'Employee_ID': emp_id,\n",
        "            'Department': random.choice(['Engineering', 'Sales', 'HR', 'Marketing']),\n",
        "            'Hire_Date': hire_date,\n",
        "            'Termination_Date': term_date, # This is the missing column\n",
        "            'Performance_Rating': random.randint(1, 5)\n",
        "        })\n",
        "\n",
        "    performance_data = []\n",
        "    for record in employees_data:\n",
        "        performance_data.append({\n",
        "            'Employee_ID': record['Employee_ID'],\n",
        "            'Performance_Cycle_Date': fake.date_between(start_date=record['Hire_Date'], end_date='today'),\n",
        "            'Performance_Rating': record['Performance_Rating']\n",
        "        })\n",
        "\n",
        "    survey_data = []\n",
        "    for record in employees_data:\n",
        "        survey_data.append({\n",
        "            'Employee_ID': record['Employee_ID'],\n",
        "            'Survey_Date': fake.date_between(start_date=record['Hire_Date'], end_date='today'),\n",
        "            'Survey_Type': 'eNPS',\n",
        "            'Score': random.randint(0, 10)\n",
        "        })\n",
        "\n",
        "    df_employees = pd.DataFrame(employees_data)\n",
        "    df_performance = pd.DataFrame(performance_data)\n",
        "    df_surveys = pd.DataFrame(survey_data)\n",
        "\n",
        "    spreadsheet = gsh.get_spreadsheet()\n",
        "    gsh.write_df_to_sheet_tab(spreadsheet, \"raw_employees\", df_employees)\n",
        "    gsh.write_df_to_sheet_tab(spreadsheet, \"raw_performance\", df_performance)\n",
        "    gsh.write_df_to_sheet_tab(spreadsheet, \"raw_surveys\", df_surveys)\n",
        "    print(\"✅ Raw data generation complete and uploaded to Google Sheet.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_raw_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC7LDx37JSmq",
        "outputId": "54683df4-b820-4ec0-fa9e-c61675387561"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting raw data generation with complete schema...\n",
            "Successfully wrote data to tab: 'raw_employees'\n",
            "Successfully wrote data to tab: 'raw_performance'\n",
            "Successfully wrote data to tab: 'raw_surveys'\n",
            "✅ Raw data generation complete and uploaded to Google Sheet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "streamlit\n",
        "pandas\n",
        "plotly\n",
        "gspread\n",
        "gspread-dataframe\n",
        "oauth2client\n",
        "Faker"
      ],
      "metadata": {
        "id": "HbtshblDDp_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%%writefile model_trainer.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "import google_sheets_handler as gsh\n",
        "\n",
        "def process_data_and_train():\n",
        "    print(\"Reading raw data from Google Sheet...\")\n",
        "    spreadsheet = gsh.get_spreadsheet()\n",
        "    df_employees = gsh.read_sheet_tab_to_df(spreadsheet, \"raw_employees\")\n",
        "    df_performance = gsh.read_sheet_tab_to_df(spreadsheet, \"raw_performance\")\n",
        "    df_surveys = gsh.read_sheet_tab_to_df(spreadsheet, \"raw_surveys\")\n",
        "\n",
        "    # Ensure consistent data types for the merge key\n",
        "    df_employees['Employee_ID'] = pd.to_numeric(df_employees['Employee_ID'], errors='coerce')\n",
        "    df_performance['Employee_ID'] = pd.to_numeric(df_performance['Employee_ID'], errors='coerce')\n",
        "    df_surveys['Employee_ID'] = pd.to_numeric(df_surveys['Employee_ID'], errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid Employee_ID after coercion\n",
        "    df_employees.dropna(subset=['Employee_ID'], inplace=True)\n",
        "    df_performance.dropna(subset=['Employee_ID'], inplace=True)\n",
        "    df_surveys.dropna(subset=['Employee_ID'], inplace=True)\n",
        "\n",
        "    # Convert other types\n",
        "    df_employees['Hire_Date'] = pd.to_datetime(df_employees['Hire_Date'], errors='coerce')\n",
        "    df_employees['Termination_Date'] = pd.to_datetime(df_employees['Termination_Date'], errors='coerce')\n",
        "    df_performance['Performance_Rating'] = pd.to_numeric(df_performance['Performance_Rating'], errors='coerce')\n",
        "    df_surveys['Score'] = pd.to_numeric(df_surveys['Score'], errors='coerce')\n",
        "\n",
        "    # Create Tidy Table\n",
        "    # Ensure we get the latest performance for each employee\n",
        "    latest_performance_indices = df_performance.groupby('Employee_ID')['Performance_Cycle_Date'].idxmax().dropna()\n",
        "    latest_performance = df_performance.loc[latest_performance_indices].copy()\n",
        "\n",
        "\n",
        "    # Ensure we get the latest eNPS for each employee\n",
        "    latest_enps_indices = df_surveys[df_surveys['Survey_Type'] == 'eNPS'].groupby('Employee_ID')['Survey_Date'].idxmax().dropna()\n",
        "    latest_enps = df_surveys.loc[latest_enps_indices].copy()\n",
        "\n",
        "\n",
        "    # Merge the dataframes - merge the full latest performance and enps dataframes\n",
        "    tidy_df = pd.merge(df_employees, latest_performance, on='Employee_ID', how='left')\n",
        "    tidy_df = pd.merge(tidy_df, latest_enps, on='Employee_ID', how='left')\n",
        "\n",
        "    # Rename columns after merge to ensure correct names\n",
        "    tidy_df.rename(columns={'Performance_Rating_y': 'Performance_Rating', 'Score': 'eNPS_Score'}, inplace=True)\n",
        "    # Drop redundant columns after merge if they exist\n",
        "    tidy_df.drop(columns=['Performance_Rating_x', 'Performance_Cycle_Date', 'Survey_Date', 'Survey_Type'], errors='ignore', inplace=True)\n",
        "\n",
        "\n",
        "    tidy_df['Tenure'] = (pd.to_datetime('today') - tidy_df['Hire_Date']).dt.days / 365.25\n",
        "    tidy_df['Turnover'] = tidy_df['Termination_Date'].notna().astype(int)\n",
        "\n",
        "    print(\"Writing Tidy Master Table back to Google Sheet...\")\n",
        "    gsh.write_df_to_sheet_tab(spreadsheet, \"Tidy_Master_Table\", tidy_df)\n",
        "\n",
        "    print(\"Training turnover prediction model...\")\n",
        "    features = ['Tenure', 'Performance_Rating', 'eNPS_Score']\n",
        "\n",
        "    # Ensure features exist in the dataframe before selecting\n",
        "    missing_features = [f for f in features if f not in tidy_df.columns]\n",
        "    if missing_features:\n",
        "        print(f\"Warning: Missing features in tidy_df: {missing_features}. Model training may fail or be inaccurate.\")\n",
        "        # As a temporary fix, drop missing features from the list\n",
        "        features = [f for f in features if f in tidy_df.columns]\n",
        "        if not features:\n",
        "            print(\"Error: No valid features remaining for model training.\")\n",
        "            return # Exit if no features are available\n",
        "\n",
        "\n",
        "    X = tidy_df[features].fillna(tidy_df[features].median())\n",
        "    y = tidy_df['Turnover']\n",
        "    model = RandomForestClassifier(random_state=42).fit(X, y)\n",
        "\n",
        "    joblib.dump(model, 'turnover_model.pkl')\n",
        "    print(\"✅ Model trained and saved as turnover_model.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_data_and_train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaqmSTCAJ9mk",
        "outputId": "a1586d30-796c-47e7-bb43-4ee71c1816c3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading raw data from Google Sheet...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3017315193.py:26: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df_employees['Termination_Date'] = pd.to_datetime(df_employees['Termination_Date'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Tidy Master Table back to Google Sheet...\n",
            "Successfully wrote data to tab: 'Tidy_Master_Table'\n",
            "Training turnover prediction model...\n",
            "✅ Model trained and saved as turnover_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import joblib\n",
        "import os\n",
        "import google_sheets_handler as gsh\n",
        "\n",
        "st.set_page_config(page_title=\"People Analytics Dashboard\", page_icon=\"🚀\", layout=\"wide\")\n",
        "\n",
        "@st.cache_data(ttl=600)\n",
        "def load_data():\n",
        "    spreadsheet = gsh.get_spreadsheet()\n",
        "    tidy_df = gsh.read_sheet_tab_to_df(spreadsheet, \"Tidy_Master_Table\")\n",
        "    # Convert types after reading from sheets\n",
        "    for col in ['Hire_Date', 'Termination_Date']:\n",
        "        tidy_df[col] = pd.to_datetime(tidy_df[col], errors='coerce')\n",
        "    for col in ['Performance_Rating', 'eNPS_Score', 'Tenure']:\n",
        "        tidy_df[col] = pd.to_numeric(tidy_df[col], errors='coerce')\n",
        "    return tidy_df\n",
        "\n",
        "tidy_df = load_data()\n",
        "model = joblib.load('turnover_model.pkl')\n",
        "\n",
        "# --- Sidebar Filters ---\n",
        "st.sidebar.header(\"Global Filters\")\n",
        "selected_dept = st.sidebar.multiselect(\"Department\", options=tidy_df['Department'].unique(), default=tidy_df['Department'].unique())\n",
        "filtered_df = tidy_df[tidy_df['Department'].isin(selected_dept)]\n",
        "\n",
        "# --- Main Dashboard ---\n",
        "st.title(\"🚀 People Analytics Dashboard\")\n",
        "\n",
        "# --- KPI & Analytics Tabs ---\n",
        "tab1, tab2, tab3 = st.tabs([\"📊 KPI Dashboard\", \"🔮 Predictive Insights\", \"💡 Prescriptive Actions\"])\n",
        "\n",
        "with tab1:\n",
        "    st.header(\"Key Performance Indicators\")\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    # Lagging KPI\n",
        "    turnover_rate = (filtered_df['Termination_Date'].notna().sum() / len(filtered_df)) * 100\n",
        "    col1.metric(\"Overall Turnover Rate (Lagging)\", f\"{turnover_rate:.2f}%\")\n",
        "    # Leading KPI\n",
        "    avg_enps = filtered_df['eNPS_Score'].mean()\n",
        "    col2.metric(\"Average eNPS (Leading)\", f\"{avg_enps:.2f}\")\n",
        "    # Descriptive KPI\n",
        "    headcount = len(filtered_df[filtered_df['Termination_Date'].isna()])\n",
        "    col3.metric(\"Current Headcount (Descriptive)\", headcount)\n",
        "\n",
        "    # Add more KPI visuals here...\n",
        "    fig = px.histogram(filtered_df, x='Tenure', title='Employee Tenure Distribution')\n",
        "    st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "with tab2:\n",
        "    st.header(\"Predictive Analytics: Employee Turnover Risk\")\n",
        "    current_employees = filtered_df[filtered_df['Termination_Date'].isna()].copy()\n",
        "    features = ['Tenure', 'Performance_Rating', 'eNPS_Score']\n",
        "    current_employees['Turnover_Risk'] = model.predict_proba(current_employees[features].fillna(current_employees[features].median()))[:, 1]\n",
        "\n",
        "    st.subheader(\"Top Employees at Risk\")\n",
        "    st.dataframe(current_employees[['Employee_ID', 'Department', 'Tenure', 'Turnover_Risk']].sort_values('Turnover_Risk', ascending=False).head())\n",
        "\n",
        "with tab3:\n",
        "    st.header(\"Prescriptive Actions\")\n",
        "    high_risk_employee = current_employees.sort_values('Turnover_Risk', ascending=False).iloc[0]\n",
        "    st.subheader(f\"Recommended Actions for Employee ID: {high_risk_employee['Employee_ID']}\")\n",
        "    if high_risk_employee['Performance_Rating'] >= 4:\n",
        "        st.warning(\"High-performer at risk! **Action:** Schedule a career development discussion with their manager immediately.\")\n",
        "    if high_risk_employee['eNPS_Score'] <= 6:\n",
        "        st.info(\"Low engagement score detected. **Action:** Manager to conduct a 'stay interview' to understand concerns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QWAw-LtJYzB",
        "outputId": "02a3951d-9b14-466a-d6b7-02115d8ea474"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0DFtfelXzAcP",
        "outputId": "12ded611-18d9-485c-88c9-48278daf8e8d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.48.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (5.24.1)\n",
            "Requirement already satisfied: Faker in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (37.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (1.6.1)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (6.2.1)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (4.1.3)\n",
            "Collecting pyngrok (from -r requirements.txt (line 8))\n",
            "  Downloading pyngrok-7.3.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (25.0)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit->-r requirements.txt (line 1)) (6.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.6.0)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from gspread->-r requirements.txt (line 6)) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from gspread->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client->-r requirements.txt (line 7)) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from oauth2client->-r requirements.txt (line 7)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from oauth2client->-r requirements.txt (line 7)) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from oauth2client->-r requirements.txt (line 7)) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from oauth2client->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok->-r requirements.txt (line 8)) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (4.0.12)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.12/dist-packages (from httplib2>=0.9.1->oauth2client->-r requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 1)) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 1)) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit->-r requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread->-r requirements.txt (line 6)) (3.3.1)\n",
            "Downloading pyngrok-7.3.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['GGL_SHEET_KEY'] = userdata.get('GGL_SHEET_KEY')\n",
        "\n",
        "ngrok.kill()\n",
        "NGROK_AUTH_TOKEN = \"31fq6Ze7AbyVhGp5cFN3WM5HSPY_5dwBEa7toz8xUPZ9S5aPe\"  # <--- PASTE YOUR TOKEN HERE\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"🚀 Your dashboard is live at: {public_url}\")\n",
        "!streamlit run app.py --server.port 8501 --server.headless true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX1gV_BYMY8D",
        "outputId": "a2e277ba-080f-4508-860c-bc44c8deefa0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Your dashboard is live at: NgrokTunnel: \"https://16ab8c03944e.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-24T10:31:07+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.139.212.246:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-08-24T10:33:57+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-462ad296-38fd-46bc-b89f-c97d4bba4838 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-08-24T10:33:57+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-462ad296-38fd-46bc-b89f-c97d4bba4838 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O93YG1RwWkyo",
        "outputId": "8ddebb1c-52b7-4822-f427-0a4635ca5001"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backup"
      ],
      "metadata": {
        "id": "oIkycyf4yuBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# This is the same app.py file from the original response.\n",
        "# No changes are needed here.\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from datetime import date, timedelta\n",
        "from google_sheets_handler import get_sheet, read_data_from_sheet, append_data_to_sheet\n",
        "from data_generator import generate_employee_data\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"People Analytics Dashboard\",\n",
        "    page_icon=\"📊\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# --- Data Loading and Caching ---\n",
        "# @st.cache_data decorator removed for Colab compatibility with gspread objects\n",
        "def load_data():\n",
        "    sheet = get_sheet()\n",
        "    df = read_data_from_sheet(sheet)\n",
        "    if df.empty:\n",
        "        st.warning(\"Data sheet is empty. Generating initial data for the last 12 months.\")\n",
        "        today = date.today()\n",
        "        initial_data = generate_employee_data(start_date=today - timedelta(days=365), end_date=today)\n",
        "        append_data_to_sheet(sheet, initial_data)\n",
        "        df = read_data_from_sheet(sheet) # Reload data\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "\n",
        "# --- Sidebar Filters ---\n",
        "st.sidebar.header(\"📊 People Analytics Dashboard\")\n",
        "st.sidebar.markdown(\"Filter your data to get specific insights.\")\n",
        "\n",
        "# Ensure Hire_Date is datetime before finding min/max\n",
        "df[\"Hire_Date\"] = pd.to_datetime(df[\"Hire_Date\"])\n",
        "\n",
        "min_date = df[\"Hire_Date\"].min().date()\n",
        "max_date = df[\"Hire_Date\"].max().date()\n",
        "\n",
        "date_range = st.sidebar.date_input(\n",
        "    \"Select Hire Date Range\",\n",
        "    value=(min_date, max_date),\n",
        "    min_value=min_date,\n",
        "    max_value=max_date,\n",
        ")\n",
        "\n",
        "selected_departments = st.sidebar.multiselect(\n",
        "    \"Select Departments\",\n",
        "    options=sorted(df[\"Department\"].unique()),\n",
        "    default=sorted(df[\"Department\"].unique()),\n",
        ")\n",
        "\n",
        "selected_locations = st.sidebar.multiselect(\n",
        "    \"Select Locations\",\n",
        "    options=sorted(df[\"Location\"].unique()),\n",
        "    default=sorted(df[\"Location\"].unique()),\n",
        ")\n",
        "\n",
        "# --- Data Generation Sidebar ---\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.header(\"Generate New Data\")\n",
        "if st.sidebar.button(\"Generate New Month's Data\"):\n",
        "    with st.spinner(\"Generating and appending new data...\"):\n",
        "        last_date = df[\"Hire_Date\"].max().date()\n",
        "        new_data = generate_employee_data(start_date=last_date + timedelta(days=1), end_date=last_date + timedelta(days=31))\n",
        "        sheet = get_sheet()\n",
        "        append_data_to_sheet(sheet, new_data)\n",
        "        st.success(\"New data generated! Please refresh the page to see updates.\")\n",
        "\n",
        "\n",
        "# --- Filter Data based on selection ---\n",
        "start_date, end_date = date_range\n",
        "filtered_df = df[\n",
        "    (df[\"Hire_Date\"].dt.date >= start_date) &\n",
        "    (df[\"Hire_Date\"].dt.date <= end_date) &\n",
        "    (df[\"Department\"].isin(selected_departments)) &\n",
        "    (df[\"Location\"].isin(selected_locations))\n",
        "]\n",
        "\n",
        "# --- Main Dashboard Display ---\n",
        "st.title(\"📈 HR KPI Dashboard\")\n",
        "st.markdown(\"This dashboard provides an overview of key human resources metrics.\")\n",
        "\n",
        "if filtered_df.empty:\n",
        "    st.warning(\"No data available for the selected filters.\")\n",
        "else:\n",
        "    # --- Key Metrics ---\n",
        "    total_hires = len(filtered_df)\n",
        "\n",
        "    # Ensure Termination_Date is also datetime\n",
        "    filtered_df['Termination_Date'] = pd.to_datetime(filtered_df['Termination_Date'])\n",
        "    turnover_90_day = filtered_df[\n",
        "        (filtered_df['Termination_Date'].notna()) &\n",
        "        ((filtered_df['Termination_Date'] - filtered_df['Hire_Date']).dt.days <= 90)\n",
        "    ].shape[0]\n",
        "    turnover_rate = (turnover_90_day / total_hires * 100) if total_hires > 0 else 0\n",
        "    offer_acceptance_rate = (filtered_df[filtered_df['Offer_Status'] == 'Accepted'].shape[0] / filtered_df.shape[0] * 100)\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    with col1:\n",
        "        st.metric(label=\"Total Hires\", value=f\"{total_hires}\")\n",
        "    with col2:\n",
        "        st.metric(label=\"90-Day Turnover Rate\", value=f\"{turnover_rate:.2f}%\")\n",
        "    with col3:\n",
        "        st.metric(label=\"Offer Acceptance Rate\", value=f\"{offer_acceptance_rate:.2f}%\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "\n",
        "    # --- Visualizations ---\n",
        "    with st.container():\n",
        "        st.header(\"KPI Deep Dive\")\n",
        "\n",
        "        with st.expander(\"Attract & Hire\"):\n",
        "            col1, col2 = st.columns(2)\n",
        "            with col1:\n",
        "                filtered_df['Requisition_Approval_Date'] = pd.to_datetime(filtered_df['Requisition_Approval_Date'])\n",
        "                filtered_df['Offer_Acceptance_Date'] = pd.to_datetime(filtered_df['Offer_Acceptance_Date'])\n",
        "                filtered_df['Time_to_Fill'] = (filtered_df['Offer_Acceptance_Date'] - filtered_df['Requisition_Approval_Date']).dt.days\n",
        "                avg_time_to_fill = filtered_df.groupby(pd.Grouper(key='Hire_Date', freq='M'))['Time_to_Fill'].mean().reset_index()\n",
        "\n",
        "                fig = px.line(avg_time_to_fill, x='Hire_Date', y='Time_to_Fill', title='Average Time to Fill (Days)', markers=True)\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "                st.markdown(\"_A line chart shows the trend in hiring efficiency over time._\")\n",
        "\n",
        "            with col2:\n",
        "                source_counts = filtered_df['Application_Source_Channel'].value_counts().reset_index()\n",
        "                source_counts.columns = ['Source', 'Count']\n",
        "\n",
        "                fig = px.pie(source_counts, names='Source', values='Count', title='Source of Hire Effectiveness')\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "                st.markdown(\"_A pie chart is used to show the proportion of hires from each recruitment channel._\")\n",
        "\n",
        "        with st.expander(\"Talent & Leadership\"):\n",
        "            turnover_df = filtered_df[filtered_df['Termination_Date'].notna()]\n",
        "            if not turnover_df.empty:\n",
        "                turnover_by_dept = turnover_df.groupby('Department').size().reset_index(name='Turnover Count')\n",
        "                fig = px.bar(turnover_by_dept, x='Department', y='Turnover Count', title='Turnover by Department')\n",
        "                st.plotly_chart(fig, use_container_width=True)\n",
        "                st.markdown(\"_A bar chart helps compare turnover volumes across different departments._\")\n",
        "            else:\n",
        "                st.info(\"No turnover data to display for the selected period.\")"
      ],
      "metadata": {
        "id": "tP6novUqytI6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}